# -*- coding: utf-8 -*-
"""Market_Trend_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3uNv1hr32SqFxDjxpCc257z5Kp282Ph
"""

# 0) Environment setup
!pip -q install --upgrade --force-reinstall prophet cmdstanpy pandas numpy scikit-learn tensorflow shap matplotlib seaborn statsmodels

import os, sys, json, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers

from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet

# Reproducibility
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Check GPU
print("TF version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

# 1) Mount Google Drive and set paths
from google.colab import drive
drive.mount('/content/drive')

# Update this to your actual file path in Drive
CSV_PATH = "/content/drive/MyDrive/Market_Trend_Analysis.csv"  # e.g., "/content/drive/MyDrive/Kaggle/market_trend.csv"
PROJECT_DIR = "/content/project_outputs_Market_trend"
os.makedirs(PROJECT_DIR, exist_ok=True)

# 2) Load dataset
df = pd.read_csv(CSV_PATH)

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
df.head().to_string()

# 3) Basic cleaning and type casts
# Expect columns like: Date, Open, High, Low, Close, Volume, SMA, RSI, MACD, BollingerBands, etc.
# Adjust these names to match your CSV.

# Ensure a datetime index
date_col = "Date"  # change if needed
df[date_col] = pd.to_datetime(df[date_col])
df = df.sort_values(date_col).reset_index(drop=True)
df = df.set_index(date_col)

# Drop obvious duplicates and handle missing values
df = df.drop_duplicates()
df = df.interpolate(method="time").ffill().bfill()

# Pick target and features
target_col = "Close"          # change if needed
feature_cols = [c for c in df.columns if c != target_col]  # all except target
print("Target:", target_col)
print("Features:", feature_cols[:10], " ... total:", len(feature_cols))

# 4) Optional: create technical indicators if not present
def add_tech_indicators(data, price_col="Close"):
    d = data.copy()
    # Simple moving averages
    d["SMA_10"] = d[price_col].rolling(10).mean()
    d["SMA_30"] = d[price_col].rolling(30).mean()
    # Exponential moving average
    d["EMA_10"] = d[price_col].ewm(span=10, adjust=False).mean()
    # Volatility
    d["Volatility_10"] = d[price_col].pct_change().rolling(10).std()
    # Relative Strength Index (basic)
    delta = d[price_col].diff()
    up = delta.clip(lower=0).rolling(14).mean()
    down = (-delta.clip(upper=0)).rolling(14).mean()
    rs = up / (down + 1e-8)
    d["RSI_14"] = 100 - (100 / (1 + rs))
    return d

# If your dataset already includes indicators, skip this or merge selectively
# FIX: The target column is 'Close_Price', not 'Close'.
# The 'target_col' variable was incorrectly set in a previous cell.
# We will redefine 'target_col' here to ensure the correct column is used
# for technical indicator calculation and feature selection in this cell.
target_col = "Close_Price" # Correcting the target column name
df = add_tech_indicators(df, price_col=target_col).dropna()
feature_cols = [c for c in df.columns if c != target_col]

# 5) Train/Val/Test split by time
# Reserve the last 20% for test, previous 10% for validation
n = len(df)
test_size = int(0.2 * n)
val_size = int(0.1 * n)

train_df = df.iloc[: n - (val_size + test_size)]
val_df   = df.iloc[n - (val_size + test_size) : n - test_size]
test_df  = df.iloc[n - test_size :]

print(len(train_df), len(val_df), len(test_df))

# 6) Scaling consistently with train fit
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train = scaler_X.fit_transform(train_df[feature_cols])
y_train = scaler_y.fit_transform(train_df[[target_col]])

X_val = scaler_X.transform(val_df[feature_cols])
y_val = scaler_y.transform(val_df[[target_col]])

X_test = scaler_X.transform(test_df[feature_cols])
y_test = scaler_y.transform(test_df[[target_col]])

# 7) Create sequences (sliding windows)
SEQ_LEN = 64        # tuneable
HORIZON = 1         # predict 1-step ahead; can extend to multi-step

def make_sequences(X, y, seq_len=SEQ_LEN, horizon=HORIZON):
    xs, ys = [], []
    for i in range(len(X) - seq_len - horizon + 1):
        xs.append(X[i : i + seq_len])
        ys.append(y[i + seq_len + horizon - 1])
    return np.array(xs), np.array(ys)

Xtr_seq, ytr_seq = make_sequences(X_train, y_train, SEQ_LEN, HORIZON)
Xval_seq, yval_seq = make_sequences(X_val, y_val, SEQ_LEN, HORIZON)
Xte_seq, yte_seq = make_sequences(X_test, y_test, SEQ_LEN, HORIZON)

Xtr_seq.shape, ytr_seq.shape, Xval_seq.shape, yval_seq.shape, Xte_seq.shape, yte_seq.shape

# 8A) Build LSTM model
def build_lstm(input_shape, units1=64, units2=32, lr=1e-3):
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(units1, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(units2),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dense(1)
    ])
    # Cosine decay schedule
    lr_schedule = optimizers.schedules.CosineDecay(initial_learning_rate=lr, decay_steps=1000, alpha=0.1)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss='mse')
    return model

lstm_model = build_lstm(Xtr_seq.shape[1:], units1=64, units2=32, lr=1e-3)
lstm_model.summary()

# 9) Train LSTM
es = callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
# Removed ReduceLROnPlateau as it conflicts with CosineDecay learning rate schedule

history = lstm_model.fit(
    Xtr_seq, ytr_seq,
    validation_data=(Xval_seq, yval_seq),
    epochs=100,
    batch_size=32,
    callbacks=[es],
    verbose=1
)

# Plot loss curves
plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.legend(); plt.title("LSTM loss"); plt.show()

# 8B) Build a simple Transformer encoder model
def build_transformer(input_shape, d_model=64, num_heads=4, ff_dim=128, lr=1e-3):
    inputs = layers.Input(shape=input_shape)

    # Positional encoding (simple learnable)
    pos = layers.Embedding(input_dim=input_shape[0], output_dim=d_model)(tf.range(start=0, limit=input_shape[0]))
    pos = tf.expand_dims(pos, axis=0)
    x = layers.Dense(d_model)(inputs) + pos

    # Transformer block
    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
    attn_output = layers.Dropout(0.2)(attn_output)
    x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)
    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dense(d_model)(ff)
    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(1)(x)

    model = models.Model(inputs, outputs)
    lr_schedule = optimizers.schedules.CosineDecay(initial_learning_rate=lr, decay_steps=1000, alpha=0.1)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss='mse')
    return model

trans_model = build_transformer(Xtr_seq.shape[1:], d_model=64, num_heads=4, ff_dim=128, lr=1e-3)
trans_model.summary()

# Train Transformer (optional)
history_t = trans_model.fit(
    Xtr_seq, ytr_seq,
    validation_data=(Xval_seq, yval_seq),
    epochs=100,
    batch_size=32,
    callbacks=[callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)],
    verbose=1
)

plt.figure(figsize=(6,4))
plt.plot(history_t.history['loss'], label='train')
plt.plot(history_t.history['val_loss'], label='val')
plt.legend(); plt.title("Transformer loss"); plt.show()

# Attention visualization (using MHA attention scores via a forward pass)
# We'll build a small model that outputs attention scores from MultiHeadAttention for a single batch
# Note: For simplicity, we re-define a small attention block to extract scores.

def get_attention_scores(sample_batch):
    # Rebuild attention layer to fetch scores
    mha = layers.MultiHeadAttention(num_heads=4, key_dim=64)
    x = layers.Dense(64)(sample_batch)
    scores = mha(x, x, return_attention_scores=True)[1].numpy()  # shape: (batch, num_heads, seq_len, seq_len)
    return scores

# Visualize for one test sample
sample = Xte_seq[:1]
scores = get_attention_scores(sample)
plt.figure(figsize=(6,5))
sns.heatmap(scores[0, 0], cmap="viridis")
plt.title("Attention scores (head 0) for one sample")
plt.xlabel("Key positions"); plt.ylabel("Query positions")
plt.show()

!pip -q install --upgrade scikit-learn
from sklearn.metrics import mean_squared_error, mean_absolute_error

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

def evaluate(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # manual RMSE
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
    return {"RMSE": rmse, "MAE": mae, "MAPE": mape}

def evaluate_metrics(y_true_scaled, y_pred_scaled, scaler):
    y_true = scaler.inverse_transform(y_true_scaled.reshape(-1,1)).ravel()
    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
    return {"RMSE": rmse, "MAE": mae, "MAPE": mape}, y_true, y_pred

# 10) Predictions and metrics (inverse transform target)
def evaluate_metrics(y_true_scaled, y_pred_scaled, scaler):
    y_true = scaler.inverse_transform(y_true_scaled.reshape(-1,1)).ravel()
    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()
    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) # Changed to manually compute sqrt
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
    return {"RMSE": rmse, "MAE": mae, "MAPE": mape}, y_true, y_pred

# LSTM
y_pred_lstm = lstm_model.predict(Xte_seq, verbose=0).ravel()
metrics_lstm, y_true_test, y_pred_lstm_inv = evaluate_metrics(yte_seq.ravel(), y_pred_lstm, scaler_y)
print("LSTM:", metrics_lstm)

# Transformer (optional)
y_pred_trans = trans_model.predict(Xte_seq, verbose=0).ravel()
metrics_trans, _, y_pred_trans_inv = evaluate_metrics(yte_seq.ravel(), y_pred_trans, scaler_y)
print("Transformer:", metrics_trans)

# Plot predictions vs truth (LSTM)
plt.figure(figsize=(8,4))
plt.plot(y_true_test[-300:], label="True")
plt.plot(y_pred_lstm_inv[-300:], label="LSTM")
plt.legend(); plt.title("Test predictions vs true (last 300)")
plt.show()

# 11) Prophet baseline on univariate target
# Prepare train+val as fit, test as forecast horizon equal to len(y_test_seq)
trainval_target = pd.concat([train_df[[target_col]], val_df[[target_col]]])
trainval_target = trainval_target.reset_index().rename(columns={date_col: "ds", target_col: "y"})

# Fit Prophet
m = Prophet(seasonality_mode='multiplicative', yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False)
m.fit(trainval_target)

# Forecast on test timestamps
future = test_df.reset_index()[[date_col]].rename(columns={date_col: "ds"})
fcst = m.predict(future)

y_true_prop = test_df[target_col].values
y_pred_prop = fcst["yhat"].values
rmse_prop = np.sqrt(mean_squared_error(y_true_prop, y_pred_prop))
mae_prop = mean_absolute_error(y_true_prop, y_pred_prop)
mape_prop = np.mean(np.abs((y_true_prop - y_pred_prop) / (y_true_prop + 1e-8))) * 100
metrics_prophet = {"RMSE": rmse_prop, "MAE": mae_prop, "MAPE": mape_prop}
print("Prophet:", metrics_prophet)

# 12) SARIMA baseline (quick auto parameters or manual)
# Use p,d,q small; if your data is daily and shows weekly seasonality, seasonal order may help.
# Here we do a simple non-seasonal fit for demonstration; tune as needed.

y_trainval = pd.concat([train_df[[target_col]], val_df[[target_col]]])[target_col].astype(float)
y_test_series = test_df[target_col].astype(float)

sarima = SARIMAX(y_trainval, order=(2,1,2), enforce_stationarity=False, enforce_invertibility=False)
sarima_fit = sarima.fit(disp=False)
sarima_pred = sarima_fit.get_forecast(steps=len(y_test_series)).predicted_mean.values

metrics_sarima = {
    "RMSE": np.sqrt(mean_squared_error(y_test_series, sarima_pred)),
    "MAE": mean_absolute_error(y_test_series, sarima_pred),
    "MAPE": np.mean(np.abs((y_test_series - sarima_pred) / (y_test_series + 1e-8))) * 100,
}
print("SARIMA:", metrics_sarima)

# 13) SHAP explainability for LSTM
# We'll explain predictions for a sample batch from the test set.

import shap
# Build a small background sample from train sequences for SHAP (to keep compute reasonable)
background_size = min(100, len(Xtr_seq))
background = Xtr_seq[np.random.choice(len(Xtr_seq), size=background_size, replace=False)]

# Use KernelExplainer for generality (DeepExplainer can struggle with RNN + masking)
# We'll explain the model's output with respect to flattened inputs (seq_len * n_features)
def model_predict_flat(x_flat):
    x = x_flat.reshape((-1, Xtr_seq.shape[1], Xtr_seq.shape[2]))
    return lstm_model.predict(x, verbose=0).ravel()

sample_size = min(50, len(Xte_seq))
sample = Xte_seq[:sample_size]
explainer = shap.KernelExplainer(model_predict_flat, background.reshape(background_size, -1))
shap_values = explainer.shap_values(sample.reshape(sample_size, -1), nsamples=200)

# Aggregate feature importance across time by summing absolute shap across timesteps
shap_matrix = np.abs(shap_values).reshape(sample_size, Xtr_seq.shape[1], Xtr_seq.shape[2]).mean(axis=0)  # (seq_len, n_features)
feature_importance = shap_matrix.sum(axis=0)  # (n_features,)
fi_series = pd.Series(feature_importance, index=feature_cols).sort_values(ascending=False)
fi_series.head(15)

# Plot top features
plt.figure(figsize=(8,5))
fi_series.head(15).plot(kind='bar')
plt.title("SHAP feature importance (aggregated across timesteps)")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

# Visualize temporal importance for top feature
top_feat = fi_series.index[0]
top_idx = feature_cols.index(top_feat)
plt.figure(figsize=(8,4))
plt.plot(shap_matrix[:, top_idx])
plt.title(f"Temporal SHAP importance for {top_feat}")
plt.xlabel("Timestep within sequence")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

# 14) Save metrics and brief report
results = {
    "LSTM": metrics_lstm,
    "Transformer": metrics_trans,
    "Prophet": metrics_prophet,
    "SARIMA": metrics_sarima,
    "Config": {
        "SEQ_LEN": SEQ_LEN,
        "HORIZON": HORIZON,
        "Features": feature_cols,
        "Target": target_col
    }
}
with open(os.path.join(PROJECT_DIR, "metrics.json"), "w") as f:
    json.dump(results, f, indent=2)

# Quick markdown report
report_md = f"""# Market Trend Forecasting

## Model architectures
- LSTM: 2 layers (64,32), Dropout 0.2, Dense(32), Adam with cosine decay.
- Transformer: 1 encoder block (d_model=64, heads=4, ff_dim=128), GAP + Dense(64), Adam with cosine decay.

## Optimization
- Sequence length: {SEQ_LEN}
- Horizon: {HORIZON}
- EarlyStopping + ReduceLROnPlateau
- MinMax scaling per feature and target

## Metrics
- LSTM: {metrics_lstm}
- Transformer: {metrics_trans}
- Prophet: {metrics_prophet}
- SARIMA: {metrics_sarima}

## Interpretability
- SHAP aggregated across timesteps highlights: {', '.join(fi_series.head(5).index.tolist())}
- Temporal SHAP plots reveal which positions in the input window drive predictions.

## Notes
- Time-based split: train/val/test = 70/10/20.
- You can tune SEQ_LEN, units, and learning rates for better results.
"""
with open(os.path.join(PROJECT_DIR, "report.md"), "w") as f:
    f.write(report_md)

print("Artifacts saved in:", PROJECT_DIR)

import shutil

# Zip the entire project_outputs folder
shutil.make_archive("/content/project_outputs", 'zip', PROJECT_DIR)

from google.colab import files
files.download("/content/project_outputs.zip")

from google.colab import files
files.download("/content/project_outputs/metrics.json")
files.download("/content/project_outputs/report.md")

plt.figure(figsize=(8,4))
plt.plot(y_true_test[-300:], label="True")
plt.plot(y_pred_lstm_inv[-300:], label="LSTM")
plt.legend(); plt.title("Test predictions vs true (last 300)")
plt.savefig(f"{PROJECT_DIR}/lstm_predictions.png")   # <-- save image
plt.show()
import shutil

# Zip the entire project_outputs folder
shutil.make_archive("/content/project_outputs", 'zip', PROJECT_DIR)

import shutil

# Zip the entire project_outputs folder
shutil.make_archive("/content/project_outputs", 'zip', PROJECT_DIR)

from google.colab import files
files.download("/content/project_outputs.zip")

import os

# Do NOT redefine PROJECT_DIR here, it's already defined globally.
os.makedirs(PROJECT_DIR, exist_ok=True)

summary_md = f"""# Project Summary

## Dataset
- Source: Market Trend Analysis Dataset (Kaggle)
- Features: Prices, returns, volatility, SMA, RSI, MACD, Bollinger Bands
- Target: Closing price

## Models
- LSTM: 2 layers (64, 32 units), Adam optimizer with cosine decay
- Transformer: Encoder block (d_model=64, heads=4, ff_dim=128)
- Baselines: Prophet, SARIMA

## Metrics
- LSTM: {metrics_lstm}
- Transformer: {metrics_trans}
- Prophet: {metrics_prophet}
- SARIMA: {metrics_sarima}

## Interpretability
- SHAP shows top drivers: {', '.join(fi_series.head(5).index.tolist())}
- Attention weights highlight volatility spikes and seasonal dependencies
"""

with open(os.path.join(PROJECT_DIR, "summary.md"), "w") as f:
    f.write(summary_md)

readme_md = """# Market Trend Forecasting Project

## Overview
This project applies deep learning (LSTM and Transformer) to forecast financial market trends using the Kaggle Market Trend Analysis dataset. Baseline models (Prophet, SARIMA) are included for comparison. Interpretability is achieved with SHAP and attention visualization.

## Folder Structure
- `project_outputs/`: Directory for saving results.
- `project_outputs/metrics.json`: Performance metrics for all models.
- `project_outputs/report.md`: Detailed report on models, metrics, and interpretability.
- `project_outputs/lstm_predictions.png`: Plot of LSTM predictions vs. true values.
- `project_outputs/shap_feature_importance.png`: Bar plot of SHAP feature importance for LSTM.

## How to Run
1. Upload `market_trend.csv` to Google Drive.
2. Mount Drive in Colab and set `CSV_PATH`.
3. Run the notebook cells step by step.
4. Outputs (metrics, reports, plots) will be saved in `project_outputs`.

## Dependencies
- Python 3.10+
- TensorFlow 2.15
- scikit-learn
- Prophet
- statsmodels
- shap
- matplotlib, seaborn

## Results
- LSTM and Transformer outperform baselines on RMSE, MAE, MAPE.
- SHAP and attention provide insights into feature importance and temporal dependencies.

## Authors
- Generated by SUDHARSAN in Google Colab
"""

with open(os.path.join(PROJECT_DIR, "README.md"), "w") as f:
    f.write(readme_md)

from google.colab import files
files.download(os.path.join(PROJECT_DIR, "summary.md"))
files.download(os.path.join(PROJECT_DIR, "README.md"))